{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from models.baseline_average import BaselineAverageModel\n",
    "from models.convolved_average import ConvolvedAverageModel\n",
    "from models.empty_checkerboard import EmptyCheckerboardModel\n",
    "from reconstruct import nparray_to_mp4, mp4_to_nparray\n",
    "from score import calculate_mse, evaluate_and_save_video\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IN_HEIGHT = 2160\n",
    "IN_WIDTH = 1920\n",
    "OUT_HEIGHT = 2160\n",
    "OUT_WIDTH = 3840\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"data/Seconds_That_Count-checkerboard.mov\"\n",
    "output_path_cam = \"data/Seconds_That_Count-cvavg.mov\"\n",
    "output_path_bavg = \"data/Seconds_That_Count-bavg.mov\"\n",
    "native_path = \"data\\Seconds_That_Count-native.mov\"\n",
    "empty_checkerboard_path = \"data\\LifeSeconds_That_Count-checkerboard-empty.mov\"\n",
    "\n",
    "# input_path = \"data\\Lifting_Off-checkerboard.mov\"\n",
    "# output_path_cam = \"data/Lifting_Off-cvavg.mov\"\n",
    "# output_path_bavg = \"data/Lifting_Off-bavg.mov\"\n",
    "# native_path = \"data\\Lifting_Off-native.mov\"\n",
    "\n",
    "# input_path = \"data\\Moment+of+Intensity-checkerboard.mov\"\n",
    "# output_path_cam = \"data/Moment+of+Intensity-cvavg.mov\"\n",
    "# output_path_bavg = \"data/Moment+of+Intensity-bavg.mov\"\n",
    "# native_path = \"data\\Moment+of+Intensity-native.mov\"\n",
    "\n",
    "# input_path = \"data\\LifeUntouched_P3_4K_PQ_XQ-checkerboard.mov\"\n",
    "# output_path_cam = \"data/LifeUntouched_P3_4K_PQ_XQ-cvavg.mov\"\n",
    "# output_path_bavg = \"data/LifeUntouched_P3_4K_PQ_XQ-bavg.mov\"\n",
    "# empty_checkerboard_path = \"data\\LifeUntouched_P3_4K_PQ_XQ-checkerboard-empty.mov\"\n",
    "# output_path_s3dcnn = \"data/LifeUntouched_P3_4K_PQ_XQ-s3dcnn.mov\"\n",
    "# native_path = \"data\\LifeUntouched_P3_4K_PQ_XQ-native.mov\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model\n",
    "cavg_model = ConvolvedAverageModel(in_shape=(IN_HEIGHT, IN_WIDTH), out_shape=(OUT_HEIGHT, OUT_WIDTH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upscaled video saved to data/LifeUntouched_P3_4K_PQ_XQ-cvavg.mov\n"
     ]
    }
   ],
   "source": [
    "cavg_model(input_path, output_path_cam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bavg_model = BaselineAverageModel(in_shape=(IN_HEIGHT, IN_WIDTH), out_shape=(OUT_HEIGHT, OUT_WIDTH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing video.\n"
     ]
    }
   ],
   "source": [
    "bavg_model(input_path, output_path_bavg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(0.0)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate\n",
    "calculate_mse(native_path, output_path_bavg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(528975800.0)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_mse(native_path, output_path_cam)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_checkerboard = EmptyCheckerboardModel(in_shape=(IN_HEIGHT, IN_WIDTH), out_shape=(OUT_HEIGHT, OUT_WIDTH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing video.\n"
     ]
    }
   ],
   "source": [
    "empty_checkerboard(input_path, empty_checkerboard_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin Training\n",
      "Epoch 1, Loss: 1557.2991703073308\n",
      "Epoch 2, Loss: 54.000503236986695\n",
      "Epoch 3, Loss: 356.50955698251727\n",
      "Epoch 4, Loss: 199.53098429899663\n",
      "Epoch 5, Loss: 56.464440801665184\n",
      "Epoch 6, Loss: 61.39682021178305\n",
      "Epoch 7, Loss: 32.63146009549499\n",
      "Epoch 8, Loss: 24.75456662610173\n",
      "Epoch 9, Loss: 24.46308688789606\n",
      "Epoch 10, Loss: 205.9574350516498\n"
     ]
    }
   ],
   "source": [
    "from models.video_dataset import VideoDataset\n",
    "from models.cnn import CNNInterpolator\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNNInterpolator().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "video_dataset = VideoDataset(empty_checkerboard_path, native_path, max_frames=100)\n",
    "video_dataloader = DataLoader(video_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "print(\"Begin Training\")\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    for input_frames, target_frames in video_dataloader:\n",
    "        input_frames, target_frames = input_frames.to(device), target_frames.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input_frames)\n",
    "        loss = criterion(output, target_frames)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        del input_frames, target_frames, output\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(video_dataloader)}\")\n",
    "\n",
    "torch.save(model.state_dict(), \"cnn_interpolator.pth\")\n",
    "video_dataset.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed video saved to output_reconstructed.mov\n"
     ]
    }
   ],
   "source": [
    "from models.cnn import CNNReconstructor\n",
    "\n",
    "reconstructor = CNNReconstructor(\"cnn_interpolator.pth\")\n",
    "reconstructor.process_video(empty_checkerboard_path, \"output_reconstructed.mov\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs229",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
